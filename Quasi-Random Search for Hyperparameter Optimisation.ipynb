{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f5913b",
   "metadata": {
    "id": "f00e7266"
   },
   "source": [
    "# Quasi-Random Search for Hyperparameter Optimisation\n",
    "\n",
    "*Alexandra Stepanenko, MSc Data Science*\n",
    "\n",
    "### Contents\n",
    "\n",
    "1. [Introduction](#introduction)    \n",
    "    1.1 [Random search](#introductionrs)  \n",
    "    1.2 [Dataset](#introductionds)  \n",
    "2. [Hyperparameter optimisation program design](#design)   \n",
    "    2.1 [Basic model structure](#designbasic)  \n",
    "    2.2 [Architecture choices for the optimisation program](#designarc)  \n",
    "3. [Running the program](#run)  \n",
    "4. [Evaluating the best performing model using test data](#test)\n",
    "5. [Conclusion and evaluation](#conclusion)\n",
    "6. [References](#references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c246262a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16492,
     "status": "ok",
     "timestamp": 1643756296975,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "Yc8BD3wc2Ci1",
    "outputId": "e78bb6aa-007a-4069-a7ac-d7c865847828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy==1.7.3\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.1 MB 1.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy==1.7.3) (1.19.5)\n",
      "Installing collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed scipy-1.7.3\n"
     ]
    }
   ],
   "source": [
    "# We need to update google colab's version of scipy to use the qmc (Quasi-Monte Carlo) submodule\n",
    "\n",
    "!pip3 install scipy==1.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbaf3b9",
   "metadata": {
    "executionInfo": {
     "elapsed": 2779,
     "status": "ok",
     "timestamp": 1643756299747,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "3da2803d"
   },
   "outputs": [],
   "source": [
    "# Import required libraries and modules\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import models, layers, optimizers, regularizers\n",
    "import numpy as np\n",
    "from scipy.stats.qmc import Sobol\n",
    "from scipy.stats import qmc\n",
    "import random\n",
    "import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9835938e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18289,
     "status": "ok",
     "timestamp": 1643756318031,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "09d396be",
    "outputId": "90c7b447-1914-414d-e57e-311c4e82008c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64cf8cf5",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1643756318951,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "a4342322"
   },
   "outputs": [],
   "source": [
    "# Set graphs to a seaborn style\n",
    "sns.set_style('dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a5c665",
   "metadata": {
    "id": "ef35bed9"
   },
   "source": [
    "# 1 Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "\n",
    "## 1.1 Random search <a class=\"anchor\" id=\"introductionrs\"></a>\n",
    "\n",
    "In this project, I create a hyperparameter optimisation program based on the Random Search method by Bergstra and Bengio from their 2012 paper 'Random search for hyper-parameter optimization' [1].\n",
    "\n",
    "Random search has proven a simple but effective method of hyperparameter optimisation, having an advantage over grid search in that random search will trial a greater number of distinct values for each individual hyperparameter. Bergstra and Bengio argue that only some of the hyperparameters are important for each problem (although which hyperparameters this will be is not predictable), therefore it is an effective strategy to represent as many distinct values as possible for each hyperparameter in order to find values in the region of the optimum for the most important hyperparameters. \n",
    "\n",
    "In their experiements, which were based on simulations, Bergstra and Bengio found using quasirandom sequences (also known as low-discrepancy sequences) to choose 'random' hyperparameter values rather than pseudorandom numbers gave a slight improvement to hyperparameter optimisation results. In particular, Sobol sequences were highlighted as effective. Quasirandom sequences can be thought of as containing numbers which have equal distance ± a small error from one another. This means the distribution is not rigid as per a Grid Search. Simultaneously, quasirandom numbers are distributed much more evenly even across a range when compared to pseudorandom (or true random) numbers. This is advantageous as the nature of pseudorandom numbers means the distribution can risk creating blind spots of unsearched spaces. In this hyperparameter optimisation program I will use quasirandom numbers rather than pseudorandom numbers.\n",
    "\n",
    "We can create Sobol sequences using the SciPy library. Technically, the sample size of a Sobol sequence should be a power of two (i.e. $n = 2^{m}$) to ensure the full balanced properties of the sequence. However, by using a scrambled sequence, we can create a sample size of any arbitary $n$ [2]. For our purposes, it is not critical for the Sobol sequence to be perfectly balanced - sequences generated with scrambling and using any arbitary $n$ will still have low discrepancy compared to pseudorandom number generators (for example, NumPys's Random Generator). We can demonstrate this graphically.\n",
    "\n",
    "First let's have a look how the distribution of numbers look when using a pseudorandom generated set of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23af2216",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 1975,
     "status": "ok",
     "timestamp": 1643756438222,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "9023acd6",
    "outputId": "405d2548-3ffe-4ccc-fce5-f00f2c370bb1",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using NumPy to create a sequence of pseudorandom numbers\n",
    "random_generator = np.random.default_rng(seed=1)\n",
    "\n",
    "# Create 50 sets of random value pairs using NumPy, scaled to between 0 and 10 using SciPy's scale function\n",
    "rand_coords = qmc.scale(random_generator.random((2, 50)), [0 for n in range(0,50)], [10 for n in range(0,50)])\n",
    "random_sample = pd.DataFrame({'Var 1': rand_coords[0], 'Var 2': rand_coords[1]})\n",
    "\n",
    "g = sns.JointGrid(ratio = 10)\n",
    "sns.scatterplot(data = random_sample, x = 'Var 1', y = 'Var 2', ax=g.ax_joint)\n",
    "sns.scatterplot(data = random_sample, x = 'Var 1', y = 1 , ax=g.ax_marg_x)\n",
    "sns.scatterplot(data = random_sample, y = 'Var 2', x = 1 , ax=g.ax_marg_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e78b1c",
   "metadata": {
    "id": "c1d25073"
   },
   "source": [
    "We can see that there are some large gaps, and some sets of points are clustered closely together. Next let's have a look at the distribution when using quasirandom numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd9a2946",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "executionInfo": {
     "elapsed": 2170,
     "status": "ok",
     "timestamp": 1643756442671,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "38748d9f",
    "outputId": "d4a87277-8a16-41ae-9d4a-17b92f27bef1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/scipy/stats/_qmc.py:1078: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  warnings.warn(\"The balance properties of Sobol' points require\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9f41888610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAG2CAYAAAD1IGg0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3BU9f3/8dfmQi4QxMCSiKG2IA5IhNpqGyraGpRbwiQEHKWjHVEHx/4UFWVGYFC/MsqM2o5l6lgzfhX5o2OnSskIlPo1YOO3ErXKwCjYgn5rgYEQqEKALNlcfn8QFgK7m93k3D7nPB9/sdnL+ey5vTnn8/m8NtTV1dUlAAAMluF2AwAA6C+KGQDAeBQzAIDxKGYAAONRzAAAxstyuwGpaG5ucbsJABBI4XCB201ICVdmAADjUcwAAMajmAEAjEcxAwAYj2IGADAexQwAYDyKGQDAeBQzAIDxKGYAAONRzAAAxqOYAQCMRzEDABgv88knn3zS7Ub05uTJtr69MSQ1R9r176MRKSNDAwdkqLk1yePsjLjv3/NNq453dEqhkPKyEtT/85d17mfFey7Z63v7vGTP9/c7p/O9rHg+wWszszJ0PNrZ93b3Z12kuy37+11S/W6pLDudtqe4/7u+D/anLemsuxTW07nb8oLtmsr2cGkfTHk9xzFwYE6f3uc0I1Lz+yQkNe47psVv7VAk2qnc7AytqCrVb7fs1tdHWuM+fm7OBJWVDJa64r//wSljNGrYQP2geNDp1yRZVuyzdOFzv7n1+2pr74z/+gTLTun5kYPVuLcf3zmFdWhJO+Mt75zXXpw/QL+YdJl+U79bkWinLhuap/tvHKPldZ/1/jkJ2n7+ZyT8zDjrMNm2jNuOJN+lz9sklXWZxn7Y6zr1yj7o4n54/no6d1vG2669bQ+39sGUjxnDhbq6ujz/1fryEzDNkXbV1DYqEu2M/S03O0N3Tx6lF7fsSfh47YIyhXOzEr5/wQ2jNH3scIVzs3pd1toFZZJ0wXMLp1yu2oav4r4+2bJ7e/73d/9YP//vD/v8nVNdh/1tZ7zlnfva/3fj5frv/z27fs5/3Fu747U91c9MtA4Tbct47Uj2Xfq6TVJZl+nsh72tU6/sg6l8d7v2w/PXU6J/p7o93NoHe/vOveEnYFx2+ERbjw0pSZFop0Kh5I8Pn2hL+v7OLsVe09uyDp9oi/tcZ5cSvr63z0v2fFNLpF/f+Xx9bUeqzydaVijUc/2c/7i3dsdbdqqfmWgdJtqW8dqR7LuceU+62ySVZaezH/a2Tr2yD/anLf3dD6We6ynRvxN9plf2wd6+s1/4tpgNG5ij3PPuEedmZ+jc69B4j4cNHJD0/RkhxV7T27KGDRwQ97nMkBK+vrfPS/Z8cUFuv77z+frajlSfT7aseO9L5XN6a3tvj4sSrMNE2zJeO1L5Luluk1SWne5+aMI+2J+2WLEfnnltb/+O95le2gcTvc5PfFvMwnmZem7OhNgGPXOvfv2O/QkfPzdngsJ5WQnf/+CUMRp/yeDYa5It68xnxXvuyksGJ3x9b5+X7PmSgux+fedU1qEV7Yy3vHNf+9Yn+/TglDGx9729fb9WVJWm9DmJ2n7+ZyT6zJFx1mGybRmvHcm+S1+3SSrLTmc/7G2demUf7E9b+rsfnr+ezt2W8bZrb9vDrX0w1fVsOt/2mUk6PZqntV2HT7Rp2MABCudnqflkksd5WRd0oja3tutAyynlD8hUYV62CnMyEw6W6LGscz8r3nNK8vrePi/Z8/39zul8LyueT7CsooIcdXR29b3d8Zad6rpId1v2sj/0+l1S/W6pLDudtqe4/7u+D/anLemsu0SvPae9527LC7ZrKtvDpX0w5fUchyl9Zv4uZgCAfjGlmPn2NiMAIDgoZgAA41HMAADGo5gBAIxHMQMAGI9iBgAwHsUMAGA8ihkAwHgUMwCA8ShmAADjGRFnBQBAMlyZAQCMRzEDABiPYgYAMB7FDABgPIoZAMB4RvyGdltbu44ebXW7GQAQOIl+nNON83KyHwo14sosFAq53QQAwDm8dl42opgBAJAMxQwAYDyKGQDAeLYVsyVLlmjSpEmqrKyM/e3bb7/V/PnzNXXqVM2fP19Hjx61a/EAgACxrZjV1NTolVde6fG32tpaTZo0Se+8844mTZqk2tpauxYPAAgQ24rZtddeq4suuqjH3+rr61VdXS1Jqq6u1rvvvmvX4gEAAeJon9mRI0c0fPhwSVI4HNaRI0ecXDwAwKdcGwASCoU8N08BAGAmR4vZ0KFDdejQIUnSoUOHVFhY6OTiAQA+5WgxKy8v17p16yRJ69at05QpU5xcPADAp2z7pelFixbpo48+0jfffKOhQ4fqgQce0E033aSHHnpIBw4c0IgRI/TCCy9oyJAhvX5WNNqhb789aUczAQBJJMpDdOO8nCyb0bZiZiWKGYB+CUnNre06fKJNwwbmKJyXKXn+zOcNlhUzC7ZBsmJmRGo+APRZSGrcd0yL39qhSLRTudkZem7OBJWVDKagOcWBbUCcFQBfa25tj51EJSkS7dTit3aoubXd5ZYFhxPbgGIGwNcOn2iLnUTPiEQ7dfhEm0stCh4ntgHFDICvDRuYo9zsnqe63OwMDRs4wKUWBY8T24BiZqqQ1Bxp164jJ9Uc6ZCYfw7EFc7L1HNzJsROpmf6a8J5DBlwihPbgK1pIjq0gdR1SWUlg7V2QVn3SLoBp0+iHCuOGpCVoQU3jFJnl5QROv3YSgzNN1BzpF01tY097kHnZmdo7YIyhXP5/wkA61gxNN+qc1ayofncZjQQHdoATMIAEMRFhzYAkzAABHHRoQ3AJE6cs+gzM1WPaBg6tNGN2CZYzLI4qwxpb0tUTS0RFRfkqqQgW+rs/W2ptEViNKO5uqRwbtbZzlNOWGCUK7wqJDXuJc4KQAqIbYJXEWcFIGWMcoVXMZoRQMqMGeVKek3gMJoRQMqMGOXa3a9XU9uoX6z+u2pqt6px3zEKms+F8zO1oqq0x765oqpU4XxGMwKIx+OjXEmvMY9VCSD3vbFNlRMuVSgkdXVJ63fs10u3XW1ZAgh7D+AnHh/lmqzvhGLmX4dPtOnrI616ccueC/5u1XbnNiMAxxjTrwdL0WcGwFeM6NeD5UgA6dbnPjPSEADv8Xi/HnoiAcRtpCEA3uTxfj3YgASQviMNAQC8gQSQfiANAQC8wYnzsW9vM54ZPXP+fBZGTQGwBH3yKRs2MEeXDc2LzTOTpLe377f0fOzbYnZm9Mz592jpbAbQb/TJpyWcn6n7bxyj5XWfxdZXLAEkzUEgiQRoNCOjpgBYI0hJJlYlgFixvoI5mlFi1BQAW5Bkkh4n1pdvB4AAgF1IMkkPCSAA4EEkmaSHBJBupOYHECPF4HUB6ZO3IwGkqCBXI0kAge8xUgwmoE8+dSSAIIhIbwH8hQQQBBLpLYC/OHFMU8zgOYwUA/yF0YwIJEaKAf7CaMZujGYMoICMFAO8zrLRjBYc04xmhHmsGCnG8H7Ao0KWfyLFDP7E8H7AOxw4HrnNCF8KUhAsHBTAq32rgobve2PbBT8B89JtVxM0DCRDECwsx9V+nx2NRHXrNd/Rqs27Y+tuYfkYHY1ECRoGkmF4P6zGZP6+y8nOihUy6fS6W7V5t3KyrfuPJcUMvsTwfliNyfx9dzwSjbvujkeili2DIxv+1CWVlQzW2gVlDO+HJc5c7Z/fD8vVfu+cWHdcmcG/uof3jxuaf/q+PIUM/cDVft8xaboboxkBeEIAJ/MzaRoA/Iaffek7m9cdtxkBAMajmAEAjEcxA5wWOp2IsOvISTVHOuyIqQO8x+b9nj4zwEmkSCCIHNjvuTIDHESKBILIif2eYgY4iBQJBJET+z3FDHAQmZEIIif2e4oZ4CBSJBBEJIB0IwEEvhLAFAmYiwQQAPGRIoEgsnm/d6WYrV69Wn/84x8VCoV0xRVXaOXKlcrJyXGjKQAAH3C8z6ypqUlr1qzRW2+9pfXr16ujo0MbNmxwuhkAAB9xZQBIR0eHIpGI2tvbFYlENHz4cDeakRrSGgCg//yWAFJUVKS77rpLN954o3JycnTddddp8uTJTjcjNaQ1AED/+TEB5OjRo6qvr1d9fb3ef/99tba2qq6uzulmpIS0BgDoP18mgHzwwQcqKSlRYWGhsrOzNXXqVG3bts3pZqSEtAYA6D9fJoCMGDFC27dvV2trq7q6urR161aNHj3a6WakhLQGAOg/XyaATJw4UdOmTdPs2bM1a9YsdXZ26tZbb3W6GSkhrQEA+o8EkG6uJoCQ1gAgwEgA8QvSGgCg/2w+lxI0DAAwHsUMALzGj2ENfps0DQBIwo9hDQ58JwaAoH96dOrmKJyXae4BB3hAc6RdNbWNPeZl5WZnaO2CsrP9TQ6yYgBIc6Rd972xTZUTLlWo+4rs7e379dJtV6f1nRgAAnv48X+QgMuSTTB2o5hZ4Wgkqluv+Y5Wbd4dO1csLB+jo5GoZd+JPjP0GXFfgPX8GNaQk50VK2TS6XPFqs27lZNtXXGmmKHPiPsCrOfHsIbjkWjcc8XxSNSyZZi7duC6M/+DPP/evsn/gwRc1yWVlQzW2gVlvglrcOJcwZUZ+syP/4MEPKF7gvG4ofmn+5QMLmQScVYxjGb0MOK+AF8jzgrBQNwXgFQQZwUAQHIUMwCA8ShmQFD5Mf8P3kU2IwDLkd4CJzmwv3FlBgQQ6S1wkhP7G8UMCCDSW+AkJ/Y3ihkQQH7M/4N3ObG/UcyAACK9BU4iAaQbCSCADUhvQQpIAAHgbaS3wEkkgAAAkBzFDABgPIoZkiMlAvAWU4/JDGnviaj+frBF+05ELa8+9JkhMVIiAG8x9ZjMkN77v2+1vO6zWLtXVJXqZ98bInX2/vYUFwHER0oE4C2mHpN7W6KxQiadbvfyus+0tyVq2TIoZkiIlAjAW0w9JptaInHb3dQSsWwZFDMkREoE4C2mHpPFBblx211UkGvZMihmSIiUCMBbTD0mSwqytaKqtEe7V1SVamRBtmXLIAEEyZESAXiLw8ekZQkgGaf7zppaIioqyD1dyNIc/JEsAYRiBgBIyLJiZmNbJG4zAgB8gGIGADAexQzBZWqSAuzFfmEPEkAAG5iapAB7sV/YgwQQwB6mJinAXuwX9iABBLCJqUkKsBf7hT1IAAFsYmqSAuzFfmEPEkAAm5iapAB7sV/YgwSQbkyahi1IN0E87Bc9kABiIYoZALiDBBAAABxCMQMAGI9iBgAwHsXM74jmAeAFNp+LGG/qZ0TzAPACB85FXJn5GNE8ALzAiXMRxczHiOYB4AVOnIsoZj5GNA8AL3DiXEQx8zGieQB4gRPnomAngPSIrclROC/TfwMjiOYB0A+mxFkF97/oQRnp1yWFc7MUzs2KPQYAR4Wkxr2MZrQFI/0AwBmMZrQRI/0AwBmMZrQRI/0AwBmMZrQRI/0AwBmMZuzmzGhGRvoBwPksG81owfnWc79nduzYMS1cuFDTp0/XjBkztG3bNjeaERvpN25o/unRfhQyoP8It0avrN8pXLmn9vTTT+v666/XqlWr1NbWpkgk4kYzAFgtKFNekB4/Bg23tLTo448/1ty5cyVJAwYM0ODBg51uBgAbMOUF8fhyaP6+fftUWFioJUuWqLq6WsuWLdPJkzb0hwFwHFNeEI8vh+a3t7dr586dmjdvntatW6e8vDzV1tY63QwANmDKC+Lx5dD84uJiFRcXa+LEiZKk6dOna+fOnU43A4ANmPKCeJzYLxzfw8LhsIqLi/XVV19p1KhR2rp1q0aPHu10MwDYoUsqKxmstQvKmPKCsxzYL1yZZ7Zr1y4tW7ZM0WhUI0eO1MqVK3XRRRclfL1t88wAAElZNs/MxrZIQZ80DQBIypRiFtg4KwCAf1DMAADGo5gBgFOCHPVl83dnvCwAOCHIUV9+jLMCgCAKctSXL+OsACCIghz15cs4KwAIoiBHffkyzgoAgijIUV/80nQ3Jk0D8AUDf93elF+a9v9/CQDAK7p/3T6cmxV7HBg2f3duMwIAjEcxAwAYj2IGADAexQywQ5Bji4B4iLMCDBPk2CIgHuKsAPMEObYIiIc4K8BAno0t4tYnXOLEMcFtRsBiZ6J7zj14XY8t4tYnXDRskP3HBFdmgMW8GFvErU+4KTMkPThlTI9j4sEpY5SZYd3tAa7MAKt1SWUlg7V2QZlnYouS3eaJJTIANmlqOaU1W7/W3ZNHKRSSurqkNVu/1vjiAhUOzbdkGezFgB08FlvkyVufCIxhA3P0zck2vbhlT+xvjt1mPHDggB5++GH9/Oc/1+9+9ztFo9HYc7/85S8tawAA+3nx1ieCw9XU/Pnz52vq1Kn6/ve/rzfffFOff/65XnrpJV188cWqrq7WunXrLGtEb0jNByxgYGI73GdKan7CK7P//Oc/mjdvnsaNG6fly5dr3rx5uv322/Xvf/9boRBjem3HMGpYrfvW57ih+advf1LI4BrrT2gJr/Ha29t16tQp5eTkSJKqqqoUDod19913q7W11fKG4BwMowbgJ24mgNxyyy3avn17j7/95Cc/0W9+8xuNGTPGmqUjLoZRx8GVKmCs5tZ2Pf8//9Ddk0fp/vLLdc/1o/T8//zD0nNawiuzO++8M+7fr7zySr322muWNQAXYhj1ebhSBYx2NBLVrdd8R6s2744dwwvLx+hoJGrZOY1J0x50Zhj1uYI8jJorVcBsOdlZsUImnT6GV23erZxs6/5zTjHzIIZR9+TZrEMAKTkeicY9ho9Hognekb6kZ8fOzk5t2rRJM2fOtGyBSIEHEyTcxIRfwGxOHMNJr8wyMjL0yiuvWLYwpIFh1DFcqQJmc3XS9BnPP/+8Lr74Ys2cOVN5eXmxvw8ZMsSyRvSGSdNgwi/gDlMmTfdazMrLyy98Uyik+vr69FrRDxQzAHCHZcXMxrZIKQQNb9682dLGAABgtZRuWP7zn//Unj171NZ2dvRYdXW1bY0CACAdvRaz3/72t/rwww/15Zdf6qc//akaGhr0wx/+0Mxi1uOebY7CeZn0uwCwD+ecszKkvS1RNbVEVFyQq5KCbKmz97elqtdi9pe//EV1dXWqrq7WypUrdfjwYS1evNi6FjiFFAkATuKcc1aG9N7/favldZ/F1sWKqlL97HtDLCtovU6azsnJUUZGhrKysnT8+HENHTpUBw4csGbpDiJFAoCTOOectbclGitk0ul1sbzuM+1tcWjStCSVlpbq2LFjuuWWW1RTU6P8/HxdffXVljXAKeQdAnAS55yzmloicddFU0tEIwdmW7KMhGv0v/7rv1RZWaknn3xSkjRv3jxdf/31On78uMaOHWvJwp1EigQAJ3HOOau4IDfuuigqyLVsGQlvM373u9/Vs88+q/Lycj377LPauXOnSkpKjCxkEikSAJzFOeeskoJsragq7bEuVlSVamSBNVdlUgqTpvfv368NGzZo48aNikQiqqysVEVFhb73ve9Z1ojeWDY5jxQJAE7ywTnHsknT54xmLCrIPV3I0hz80a8EkHPt3LlTS5cu1T/+8Q/t2rUrvVb0AwkgAOAO3ySAtLe3q6GhQRs2bFBjY6N+9KMf6f7777e0gQAA9EfCYva3v/1N69evV0NDg6666ipVVFRoxYoVys/Pd7J9AAD0KmExe/nllzVr1iw99thjuuiii5xsEwAAaUmrz8wt9JkBIhoJrrDnJ2D6tv/2q88MgAcQjQSTObD/9hpnBcB9RCPBZE7svxQzwADJopEAr3Ni/6WYAQY4E410rqBGI8E8Tuy/FDPAAEQjwWRO7L+MZgRM4YNoJJjHntGMfdt/Gc0I+EGXFM7NOvvzIRQymMTm/ZfbjAAA41HMAADGo5gBMF9Iao60a9eRk2qOdEghtxuEC9i8jegzA2A20lG8z88JIB0dHaqurta9997rVhMA+ADpKN7n6wSQNWvWaPTo0W4tHoBPkI7ifb5NADl48KDee+89zZ07143FA/AR0lG8z7cJIM8884wWL16sjAzGnwDoH9JRvM+JbeT41t6yZYsKCwtVWlqqDz/80OnFA/CbLqmsZLDWLigjHcWrHNhGjhezTz/9VJs3b1ZDQ4NOnTql48eP69FHH9Xzzz/vdFMA+AXpKN5n8zZyNZvxww8/1KuvvqqXX3456evIZgQAd1iWzWhjWyQmTQMAfIDUfOB8PdK9cxTOy+S2FQLLntT8vh1XpOYDqSJNArCenxNAAC8iTQKwnq8TQAAvIk0CsJ4TxxW3GWEvw/qfziQVnHvgkSYB9M+wgTm6bGieKidcqlB3Wv7b2/dbelxRzGAfA/ufziQVnN9mJuECfRfOz9T9N47R8rrPYsfViqpShfOzpM7e358KRjPCNs2RdtXUNl5wlbN2QdnZiZNe1ONqkjQJBJsVoxmtOhcwmhGuSHaf3NPFjDQJwFJOnAsYAALbkGYOQPJxaj6CgTRzAJIz5wL6zGAv+p8Ao9mTANK3cwF9ZnAP/U8AJNvPBdxmBAAYj2IGADAexcwqodNzKXYdOanmSIcUcrtBAOAhNp8j6TOzgoFJFwDgGFLzzUDSOgAkRmq+IUhaB4DEnDhHUswsQNIFAol+YqSIBBBDkHSBwOnuA6mpbdQvVv9dNbVb1bjvGAUNcYXzM7WiqrTHOTKWmm8REkCsQtIFAsTYX0RA2qxKzb/vjW2x3zPr6pLW79ivl267mtR8zyHpAgFi7C8iwBWHT7Tp6yOtenHLngv+Tmo+ANfQT4x00GcGwJPoJ0Y6SM3vZkSfGRA09BMHAqn5AE7rcRDnKJyX6Y+TPv3E6DPrh71SzAA7EXUGEGcFmI6oM4A4K9iB1AZHEXUGOHMccJsxSLjl5bgzQ5LPn1zMEHYEybBB9h8HXJkFCLe8nMcQdkDKDEkPThnT4zh4cMoYZWZYd2uIIypASG1wQZdUVjJYaxeUMYQdgdXUckprtn6tuyePisVZrdn6tcYXF6hwaL4ly+AMFiDc8nIJQ9gRcMMG5uibk2094qy4zYg+45YXADeQANKNBBALkdoAIA0kgMCbuOUFpMevCS6uIgEEAJzDdBZrkAACAO5hOos1SAABABeR4GINJ9YjxQwAEuBHSK3Bj3MCgIuYzmINhuZ3Y2g+ANcEfDoLQ/MBwA+YzmINm9cjtxkBAMajmAEAjEcx8xt+fBOAF9l8bqLPzE9IKwDgRSSAIB2kFQDwIhJAkBbSCgB4EQkgSAtpBYainxM+RwII0kJagYG6+xJqahv1i9V/V03tVjXuO0ZBg6+E8zO1oqq0x7lpRVWpwvkkgCCRgKcVmKY50q6a2sYet2ByszO0dkHZ2cmlgIusSABpjrTrvje2qXLCpQqFpK4uaf2O/XrptqvT2s9JAAkS0gqMkqwvgWIGvzh8ok1fH2nVi1v2XPB3q/ZzbjMCLqKfE0FAnxngc/RzIghIze9Gnxks0aM/MUfhvExv3IalnxMeZllqfoa0tyWqppaIigtyVVKQLXX2/rZU2iLRZ4ag8HI6Cv2c8LuQ1LiXBBCg30hHAdzjywSQAwcO6I477tDMmTNVUVGh119/3ekmIIBIRwHc48Tx5/htxszMTD322GMaP368jh8/rjlz5ui6667T5Zdf7nRTECBnRlOdP5+LUYOA/Zw4/hy/Mhs+fLjGjx8vSRo0aJBGjRqlpqYmp5uBgGHUIOAe349m3Ldvn26//XatX79egwYNSvg6RjPCEowaBNJm2WhGC44/T45mPHHihBYuXKilS5cmLWSAZRg1CLjH5uPPldGM0WhUCxcu1KxZszR16lQ3mgAA8BHHi1lXV5eWLVumUaNGaf78+U4vHgDgQ44Xs08++UR1dXVqbGxUVVWVqqqq9Ne//tXpZgAAfIQ4KwDwI4vi2+wZANK39nhyAAgAwCZei29zoD3EWQGAz3gtvs2XcVYAAHt5Lb7NifZQzADAZ7z2o6/8OCcAIG1ei2/zfZxVqhjNCABpsii+jTgrAIB7vBbf5sc4KwAArEQxAwAYj2IGIL6Q1Bxp164jJ9Uc6ZBCbjcIRrN5f6LPDMCFvJYgAbORAALADV5LkIDZSAAB4AqvJUjAbCSA+AV9DzCM1xIkYDYSQPyg+15xTW2jfrH676qp3arGfccoaPA0ryVIwGzh/EytqCrtsT+tqCpVOJ8EEGM0R9pVU9vY4xI7NztDaxeUnZ08CHiRRQkSMJsVCSDNkXbd98Y2VU64VKGQ1NUlrd+xXy/ddnVa50ESQFyU7F4xxQye5rUECRjr8Ik2fX2kVS9u2XPB3606D3Kb0Wb0PQAIOifOg9xmtFui+RUjB6v5ZP9/0hwA7GRJ0HBI+vTgcX1+4Jg6u6TMkHTlJYP1g+JBaZ33uM3opi6prGSw1i4oO9v3kJ+lxr1MSAUQHG3tnapt+KrHOc9K3GZ0Qnffw7ih+QrnZqn5JBNSAQQHk6Z9igmpAIKESdM+xaAQAEHCpGmfYkIqgCBx4pzHaEa3MCEVgAEsGc0oWXLOYzSjFzEhFUCQ2HzO4zYjAMB4FDMAgPEoZkA6+DkfoG9sPnboMwNS5cBPvwO+5MCxw5UZkCInUgwAPyIBBPAQkluAviEBBPAQkluAviEBBPAQkluAviEBpJsvE0BgJpJbEDAkgAB+RHIL0DckgAAIPOb3mY95ZgACjfl95nNgG9JnBsDTmiPtqqlt7DG0Ozc7Q2sXlJ29ZWWqHv1IOQrnZXquQFvRZ9Ycadd9b2xT5YRLFeq+Int7+369dNvVaW1D+swAGCvZHCWji1mArjiPRqK69ZrvaNXm3bHvurB8jI5GopZtQ/rMAHiaX+f3BSlRJic7K1bIpNPfddXm3crJtu4/IxQzAJ7m1/l9QUqUOR6Jxv2uxyNRy5Zh9t4AwP+6pLKSwVq7oMxX8/vOXHGe3xdo+hVnPE58V67MAHhf9xylcUPzT/exGF7IJP9eccZDAkg3RjMC8CUDEmVIAAEAJBekRBkSQAAASI5iBu8gsgjwL+KsEAgBmkAKBA5xVqcxAMT/fB1ZFHQGRDYhMeKsgDT4NrIo6LjihoizQoD4NbIo6IIU2YTEiLNCYARpAmmQBCmyCYkRZ4Xg8GlkUdAFKcv4LkoAAAeNSURBVLIJiRFnhWDxYWRR0HHFDYk4qxhGMwIGMyCyCYkRZwUAUrAim5AYcVYAACTnSjFraGjQtGnTdPPNN6u2ttaNJqSGeCUAsIbf4qw6Ojr01FNP6bXXXlNRUZHmzp2r8vJyXX755U43JTkmewKANRw4nzp+ZbZjxw5ddtllGjlypAYMGKCKigrV19c73YxeMdkTAKzhxPnU8WLW1NSk4uLi2OOioiI1NTU53YxeMdkTAKzhxPmUASAJEK8EANZw4nzqeDErKirSwYMHY4+bmppUVFTkdDN6xWRPALCGLydNt7e3a9q0aVq9enVsAMivfvUrjRkzJuF7XJs0zWRPAAHHpOlEC8zK0uOPP6577rlHHR0dmjNnTtJC5iomewKANWw+nxJnBQBIyLIrMxvbIjEABADgAxQzAIDxKGYAAONRzAAAxqOYAQCMRzEDABiPYgYAMB7FDABgPIoZAMB4FDMAgPGMiLMCACAZrswAAMajmAEAjEcxAwAYj2IGADAexQwAYDyKGQDAeBQzAIDxfFnMGhoaNG3aNN18882qra11uzmOOnDggO644w7NnDlTFRUVev31191ukms6OjpUXV2te++91+2muOLYsWNauHChpk+frhkzZmjbtm1uN8lRq1evVkVFhSorK7Vo0SKdOnXK7SbZbsmSJZo0aZIqKytjf/v22281f/58TZ06VfPnz9fRo0ddbKF9fFfMOjo69NRTT+mVV17Rhg0btH79eu3Zs8ftZjkmMzNTjz32mDZu3Kg//OEP+v3vfx+o73+uNWvWaPTo0W43wzVPP/20rr/+em3atEl1dXWBWhdNTU1as2aN3nrrLa1fv14dHR3asGGD282yXU1NjV555ZUef6utrdWkSZP0zjvvaNKkSb79D77vitmOHTt02WWXaeTIkRowYIAqKipUX1/vdrMcM3z4cI0fP16SNGjQII0aNUpNTU0ut8p5Bw8e1Hvvvae5c+e63RRXtLS06OOPP459/wEDBmjw4MEut8pZHR0dikQiam9vVyQS0fDhw91uku2uvfZaXXTRRT3+Vl9fr+rqaklSdXW13n33XTeaZjvfFbOmpiYVFxfHHhcVFQXyZC5J+/bt065duzRx4kS3m+K4Z555RosXL1ZGhu928ZTs27dPhYWFWrJkiaqrq7Vs2TKdPHnS7WY5pqioSHfddZduvPFGTZ48WYMGDdLkyZPdbpYrjhw5Eivk4XBYR44ccblF9gjmkR4AJ06c0MKFC7V06VINGjTI7eY4asuWLSosLFRpaanbTXFNe3u7du7cqXnz5mndunXKy8vz7e2leI4ePar6+nrV19fr/fffV2trq+rq6txulutCoZBCoZDbzbCF74pZUVGRDh48GHvc1NSkoqIiF1vkvGg0qoULF2rWrFmaOnWq281x3KeffqrNmzervLxcixYtUmNjox599FG3m+Wo4uJiFRcXx67Kp0+frp07d7rcKud88MEHKikpUWFhobKzszV16tTADYA5Y+jQoTp06JAk6dChQyosLHS5RfbwXTG76qqr9K9//Ut79+5VW1ubNmzYoPLycreb5Ziuri4tW7ZMo0aN0vz5891ujiseeeQRNTQ0aPPmzfr1r3+tsrIyPf/88243y1HhcFjFxcX66quvJElbt24N1ACQESNGaPv27WptbVVXV1fgvv+5ysvLtW7dOknSunXrNGXKFJdbZI8stxtgtaysLD3++OO655571NHRoTlz5mjMmDFuN8sxn3zyierq6nTFFVeoqqpKkrRo0SL99Kc/dbllcNry5cv16KOPKhqNauTIkVq5cqXbTXLMxIkTNW3aNM2ePVtZWVkaN26cbr31VrebZbtFixbpo48+0jfffKMbbrhBDzzwgBYsWKCHHnpIb775pkaMGKEXXnjB7Wbagt8zAwAYz3e3GQEAwUMxAwAYj2IGADAexQwAYDyKGQDAeBQzIA133HGH3n///R5/W716tZ544ok+fd7HH3+s2bNn68orr9SmTZusaCIQSBQzIA2VlZXauHFjj79t3Lixx09uJNPR0dHj8SWXXKKVK1em/H4A8VHMgDRMmzZN7733ntra2iSdDvQ9dOiQrrnmGj3xxBOqqalRRUWFVq1aFXtPeXm5nnvuOc2ePfuCq6+SkhKNHTs2sIHIgFV8lwAC2GnIkCGaMGGCGhoadNNNN2njxo2aMWOGQqGQHn74YQ0ZMkQdHR2688479cUXX2js2LGx9/3pT39yufWAf/HfQSBNFRUVsVuNGzZsUEVFhSTpz3/+s2bPnq3q6mrt3r1bX375Zew9M2fOdKWtQFBQzIA0TZkyRVu3btXnn3+uSCSi0tJS7d27V6+++qpWr16tt99+Wz/72c906tSp2Hvy8vJcbDHgfxQzIE0DBw7Uj3/8Yy1dujR2VXbixAnl5eWpoKBAhw8fVkNDg8utBIKFYgb0QWVlpb744otYMRs7dqyuvPJKzZgxQ4888oh+8IMfpPQ5O3bs0A033KBNmzbpiSeeiH0egPSQmg8AMB5XZgAA41HMAADGo5gBAIxHMQMAGI9iBgAwHsUMAGA8ihkAwHj/Hwm4Nb/JzsiaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Sobol sampler with dimension two (this means sequences will be created in list pairs of two)\n",
    "sampler = Sobol(d=2, scramble=True, seed=17)\n",
    "\n",
    "# Get a sequence of 50 scrambled numbers and scale them to be between 0 and 10\n",
    "sobol_seqs = qmc.scale(sampler.random(50), [0, 0], [10, 10])\n",
    "\n",
    "# Reformat into plottable coordinates\n",
    "sobol_x = []\n",
    "sobol_y = []\n",
    "\n",
    "for i in sobol_seqs:\n",
    "    sobol_x.append(i[0])\n",
    "    sobol_y.append(i[1])\n",
    "\n",
    "sobol_sample = pd.DataFrame({'Var 1': sobol_x, 'Var 2': sobol_y})\n",
    "    \n",
    "# Plot \n",
    "g = sns.JointGrid(ratio = 10)\n",
    "sns.scatterplot(data = sobol_sample, x = 'Var 1', y = 'Var 2', ax=g.ax_joint)\n",
    "sns.scatterplot(data = sobol_sample, x = 'Var 1', y = 1 , ax=g.ax_marg_x)\n",
    "sns.scatterplot(data = sobol_sample, y = 'Var 2', x = 1 , ax=g.ax_marg_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa33921",
   "metadata": {
    "id": "94da4739"
   },
   "source": [
    "We can see the Sobol distribution has a lot less gaps, both in the 1D space of each individual variable, and in the 2D space of variable intersection.\n",
    "\n",
    "## 1.2 Dataset <a class=\"anchor\" id=\"introductionds\"></a>\n",
    "\n",
    "### 1.2.1 Fashion-MNIST\n",
    "\n",
    "The dataset I will be testing my hyperparameter optimisation program with is Fashion-MNIST. \n",
    "\n",
    "Fashion-MNIST is a multiclass single-label classification problem with balanced classes and a large enough number of samples to use hold-out validation. As we are doing multiclass single-label classification, we will use categorical crossentropy as the loss function, with a softmax last-layer activation. As the dataset is balanced, we will use accuracy as the test performance  metric. Any other datasets with the same characteristics as Fashion-MINST outlined above could also be used with this program.\n",
    "\n",
    "### 1.2.2 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "633eb0ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1808,
     "status": "ok",
     "timestamp": 1643756542069,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "7a546f98",
    "outputId": "4679f74b-e4fc-4289-9702-eb1719883fe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "40960/29515 [=========================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "26435584/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "4431872/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Import fashion-minst from the tensorflow bank of datasets\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22c8b7a0",
   "metadata": {
    "executionInfo": {
     "elapsed": 502,
     "status": "ok",
     "timestamp": 1643756546353,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "30640522"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the images into vectors, and converting values to a float between 0 and 1\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dc179d4",
   "metadata": {
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1643756550766,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "bce0d52e"
   },
   "outputs": [],
   "source": [
    "# One-hot encode the labels\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96aec407",
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1643756554790,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "37d295d6"
   },
   "outputs": [],
   "source": [
    "# Creating training and validation datasets\n",
    "x_valid = train_images[:10000]\n",
    "x_train = train_images[10000:]\n",
    "y_valid = train_labels[:10000]\n",
    "y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f8cc2",
   "metadata": {},
   "source": [
    "# 2 Hyperparameter optimisation program design <a class=\"anchor\" id=\"design\"></a>\n",
    "\n",
    "## 2.1 Basic model structure <a class=\"anchor\" id=\"designbasic\"></a>\n",
    "\n",
    "The model created will be a sequential model with up to five hidden dense layers (and up to five drop out layers).\n",
    "\n",
    "The following will be optimised:\n",
    "\n",
    "* Number of hidden layers (up to 5)\n",
    "* Number of neurons in hidden layers\n",
    "* Activation functions\n",
    "* Learning rate\n",
    "* Optimizer momentum \n",
    "* Batch size \n",
    "* Dropout layer dropout rate\n",
    "\n",
    "The first thing we need to build our optimisation program is a function which can take the hyperparameters of a model, build and run the model, and return the history dictionary. I define such a function below. This function has a parameter `model_type`: when this is defined as 'Trials', the model will be fitted with partial training data, and validated on hold out data from the training set, with the history dictionary being returned. When `model_type` is 'Test', the model fits the full training data, evaluates on the test data and prints the test loss and accuracy.\n",
    "\n",
    "The parameters we pass to this model are:\n",
    "\n",
    "* `L1N` to `L5N` - Size of dense layers 1 to 5 - integer\n",
    "* `L3` to `L5` - True/False whether to add the 3rd, 4th or 5th dense layer - boolean\n",
    "* `D1` to `D5` - True/False whether to add dropout layers after each dense layer - boolean\n",
    "* `D1R` to `D5R` - Dropout rate for each dropout layer - float\n",
    "* `AVN` - Activation funtion\n",
    "* `OLR` - Learning rate\n",
    "* `OM` - Momentum\n",
    "* `BS` - Batch size\n",
    "* `model_type` (explained above)\n",
    "* `x_train`, `y_train`, `x_valid`, `y_valid` - training data to be used during trials\n",
    "* `train_images`, `train_labels`, `test_images`, `test_labels`, `test_epochs` - train and test data to be used during evaluation; number of epochs to use during test fitting\n",
    "\n",
    "Note: When running model trials, I set verbose to 0 during fit, instead implementing a printed message stating which model number was being trialled to avoid excessive message printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09563322",
   "metadata": {
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1643756820281,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "a7b16db1"
   },
   "outputs": [],
   "source": [
    "def modelBuilder(L1N, D1, D1R, L2N, D2, D2R, L3, L3N, D3, D3R, L4, L4N, D4, D4R, L5, L5N, D5, D5R, OLR, OM, AVN, BS,\n",
    "                 model_type, x_train=None, y_train=None, x_valid=None, y_valid=None,\n",
    "                 train_images=None, train_labels=None, test_images=None, test_labels=None, test_epochs=0):\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # First dense layer\n",
    "    model.add(layers.Dense(L1N, activation=AVN, input_shape=(28 * 28, )))\n",
    "        \n",
    "    \n",
    "    # First dropout layer\n",
    "    if D1==True:\n",
    "        model.add(layers.Dropout(D1R))\n",
    "        \n",
    "        \n",
    "    # Second dense layer\n",
    "    model.add(layers.Dense(L2N, activation=AVN)) \n",
    "        \n",
    "        \n",
    "    # Second dropout layer\n",
    "    if D2==True:\n",
    "        model.add(layers.Dropout(D2R))\n",
    "    \n",
    "    \n",
    "    # Third dense layer\n",
    "    if L3==True:\n",
    "        model.add(layers.Dense(L3N, activation=AVN)) \n",
    "    \n",
    "        \n",
    "    # Third dropout layer\n",
    "    if D3==True:\n",
    "        model.add(layers.Dropout(D3R))\n",
    "    \n",
    "    \n",
    "    # Fourth dense layer\n",
    "    if L4==True:\n",
    "        model.add(layers.Dense(L4N, activation=AVN)) \n",
    "    \n",
    "        \n",
    "    # Fourth dropout layer\n",
    "    if D4==True:\n",
    "        model.add(layers.Dropout(D4R))\n",
    "    \n",
    "    \n",
    "    # Fifth dense layer\n",
    "    if L5==True:\n",
    "        model.add(layers.Dense(L5N, activation=AVN)) \n",
    "    \n",
    "        \n",
    "    # Fifth dropout layer\n",
    "    if D5==True:\n",
    "        model.add(layers.Dropout(D5R))\n",
    "    \n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    # Compile and fit model\n",
    "    model.compile(optimizer=optimizers.RMSprop(learning_rate=OLR, momentum=OM),\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # For trials, fit with split train and validation data, return history document\n",
    "    # Use early stopping callback\n",
    "    \n",
    "    if model_type=='Trials':\n",
    "    \n",
    "        history = model.fit(x_train, y_train, \n",
    "                            epochs=100, batch_size=BS, \n",
    "                            callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)],\n",
    "                            validation_data = (x_valid, y_valid), verbose=0)\n",
    "\n",
    "        history_dict = history.history\n",
    "\n",
    "        return history_dict\n",
    "    \n",
    "    # When testing, fit with full training data and evaluate using test data - print test loss and accuracy\n",
    "    \n",
    "    if model_type=='Test':\n",
    "        model.fit(train_images, train_labels, epochs=test_epochs, batch_size=BS)\n",
    "        test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "\n",
    "        print('The test loss was:', test_loss, '\\nThe test accuracy was:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d11e6",
   "metadata": {},
   "source": [
    "## 2.2 Architecture choices for the optimisation program <a class=\"anchor\" id=\"designarc\"></a>\n",
    "\n",
    "### 2.2.1 An overview of how the main function will work\n",
    "\n",
    "The main function will take as input: \n",
    "\n",
    "* The number of models to trial\n",
    "* A dictionary of hyperparameter numerical ranges (for layer size, dropout rate, learning rate, momentum and batch size) or list of options (for activation functions) \n",
    "* The training and validation data to be used\n",
    "\n",
    "The function will assign hyperparameter values to each model based on rules which will be described shortly. All model information and variables are stored in a dataframe. Each model is fitted in turn, and the minimum validation loss and the epoch in which this was achieved, plus the maximum validation accuracy and the epoch in which this was achieved are appended to the dataframe.\n",
    "\n",
    "The function will return:\n",
    "* A dataframe of all the models trialed with full information and variables\n",
    "* A dataframe containing only the model which achieved the lowest validation loss\n",
    "\n",
    "\n",
    "### 2.2.2 Forced diversification\n",
    "\n",
    "One of the elements which occured to me as potentially problematic in a random search approach was the fact that some architectures or hyperparameter combinations are relatively unlikely to occur within a random structure, even if they may be a combination which would work very well. For example, in a random search method where the layer sizes are chosen independently of one another, we are only rarely going to see larger networks with layers of ascending or descending sizes for the same reason we don't often see a coin land on heads five times in a row, and we are unlikely to see uniformly sized layers. \n",
    "\n",
    "Neural networks are black box systems and most of the time we can only employ heuristics to guess at what the network may be doing. However, there are some heuristics which may cause us to consider that perhaps networks with explicitly ascending or descending layer sizes may perform better. Additonally, for larger networks, if the size of each layer is always chosen independently of the rest, then on average, the total number of neurons in the networks may tend to approximately the midpoint of the range defined for layer sizes. This may not be optimum network size. \n",
    "\n",
    "To address this, I have implemented in my optimisation function a regime which I call 'forced diversification', where a proportion of the models are built with explicit layer structures - uniform, ascending, descending - and the rest are left so each layer size is defined independently of the rest ('random'). \n",
    "\n",
    "For ascending and descending networks, I have further divided these into two subtypes. The 'seed' value for these structures - the starting point from which all the layer sizes are calculated - is a quasirandom number. If we decide, for example, this seed value is always the size of the first layer, we would always get relatively large ascending networks and relatively small descending networks, which may not be optimal. Instead, for both types of structure, 50% of the time the seed value will be the smallest layer size in the network, and 50% of the time it will be the largest. This will allow for greater diversity in network size.\n",
    "\n",
    "* Ascending min - quasirandom number is smallest value in an ascending network\n",
    "* Ascending max - quasirandom number is largest value in an ascending network\n",
    "* Descending min - quasirandom number is smallest value in an descending network\n",
    "* Descending max - quasirandom number is largest value in an descending network\n",
    "\n",
    "For the total number of models trialed, approximately:\n",
    "\n",
    "* One fifth will have uniform structure\n",
    "* One fifth will have ascending structure\n",
    "* One fifth will have descending structure\n",
    "* Two fifths will have a 'random' structure\n",
    "\n",
    "'Forced diversification' is also implemented in dropout layers and momentum. After each dense layer, there is a 50% chance of a dropout layer. For larger networks, this means the chance of having networks without dropout layers is small. As such, one quarter of the networks have a stipulation of no dropout layers at all. Equally, one quarter of models have no optimiser momentum, while the rest have a momentum value from within the range defined by the user when calling the function.\n",
    "\n",
    "It is important to note that some of the the decisions made above - what proportion of models should be which structure, what proportion have no momentum, etc - are arbitrary beyond the application of common sense. If one were a neural network expert building a optimisation program for general use, one may research which choices may yield the best results in the majority of applications.\n",
    "\n",
    "### 2.2.3 Categorical hyperparameters\n",
    "\n",
    "So far we have established that we will use a random search style approach, using quasirandom values instead of pseudorandom values where appropriate. However some variables we need to define are categorical, such as activation functions. We also need to consider if pseudorandomness is the best option here.\n",
    "\n",
    "We will treat the number of layers as a categorical variable when defining number of layers for each model, so using this as an example we can examine what may happen if we use pseudorandom number generation to define number of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db569100",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1643756470714,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "8e841c56",
    "outputId": "756a0873-2c83-4782-cda1-f8355e3e0850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 randomly generated numbers: [3, 3, 4, 2, 4, 5, 5, 1, 5, 4]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "int_set = random_generator.integers(1, 6, 50).tolist()\n",
    "\n",
    "print('First 10 randomly generated numbers:', int_set[:10])\n",
    "\n",
    "count_lst = [int_set.count(x) for x in [1,2,3,4,5]]\n",
    "\n",
    "sns.barplot(x = [1,2,3,4,5], y=count_lst)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe9bed",
   "metadata": {
    "id": "c6a01679"
   },
   "source": [
    "As we can see, using a random number generator means some network architectures may end up being significantly more represented than others. To solve this problem, we can create a function which, given a list of items, returns a list of length $n$ with as uniform a distribution of items as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2155b740",
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1643756476766,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "7298bd94"
   },
   "outputs": [],
   "source": [
    "def getUniformList(items_lst, n):\n",
    "    # items_lst is items to include in list, for example [1, 2, 3, 4, 5] or ['relu', 'tanh']\n",
    "    # n is length of final list\n",
    "    \n",
    "    ret_lst = []\n",
    "    \n",
    "    # Add equal number of all the items\n",
    "    for i in range(0, n//len(items_lst)):\n",
    "        for item in items_lst:\n",
    "            ret_lst.append(item)\n",
    "            \n",
    "    # If n cannot be divided perfectly by the number of items, use random number generator to add final items to \n",
    "    # list to make up the list size to n\n",
    "    if (n%len(items_lst)) != 0:\n",
    "        for i in range(0, n%len(items_lst)):\n",
    "            ret_lst.append(items_lst[random_generator.integers(0, len(items_lst), 1)[0]])\n",
    "    \n",
    "    # Finally we need to shuffle our list\n",
    "    random.shuffle(ret_lst)\n",
    "    \n",
    "    return ret_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f87632e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1643756479630,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "f35c6662",
    "outputId": "371593b2-f9fb-48f4-a39b-606bb3cf74c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 randomly generated numbers: [4, 3, 3, 1, 2, 5, 4, 3, 1, 5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_lst = getUniformList([1, 2, 3, 4, 5], 52)\n",
    "\n",
    "print('First 10 randomly generated numbers:', layers_lst[:10])\n",
    "\n",
    "count_lst = [layers_lst.count(x) for x in [1,2,3,4,5]]\n",
    "\n",
    "sns.barplot(x = [1,2,3,4,5], y=count_lst)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb92af4",
   "metadata": {},
   "source": [
    "We can use this function to ensure categorical hyperparameters have an equal split of hyperparameter options across our trials.\n",
    "\n",
    "### 2.2.4 Structure of hyperparameter selection\n",
    "\n",
    "Hyperparameter variables are defined as such:\n",
    "\n",
    "* First we define the variables which every model in the trial requires, and which can be defined in a single way for all the models: layer structure, number of hidden layers, activation function, batch size, learning rate, whether dropout layers are allowed, and whether the optimiser will have momentum.\n",
    "* We create a dataframe with a row for each model, which holds the values we have defined above. Variables which we have not yet defined have a default variable such as 0.\n",
    "* The remaining variables are defined using dataframe indicies of model subsets. For example, we retrieve all indicies for models which have a third dense layer. 50% of these models will be defined as having a dropout layer after the third dense layer, while 50% will not have a dropout layer after the third dense layer. Working this way - defining variables with consideration to other variables - means we can retain a more balanced distribution of values. \n",
    "* Ascending and descending layer size network structures have a definition such that, if an ascending network has 5 layers, layer five has five times as many neurons as layer one, layer four has four times as many neurons as layer one, etc. Some practitioners may use powers of two to define ascending/descending layers (e.g. layer sizes 8, 16, 32 etc), however with some of the seed values already being quite large, I felt this would create networks which were much too large (or small) at times. Therefore I chose to scale by simple integer multiplication or division.\n",
    "\n",
    "The main search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "470a0f2c",
   "metadata": {
    "executionInfo": {
     "elapsed": 1334,
     "status": "ok",
     "timestamp": 1643756534738,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "80049442"
   },
   "outputs": [],
   "source": [
    "# Quasi-random search\n",
    "\n",
    "def QRSearch(n_trials, hp_ranges, x_train, y_train, x_valid, y_valid):\n",
    "    \n",
    "    # We will first get a list of n (where n = number of trials) values for each hyperparameter \n",
    "\n",
    "    # Categorical-type hyperparameters\n",
    "    \n",
    "    # Layer structure\n",
    "    layer_structure = getUniformList(['Uniform', 'Uniform', 'Ascending min', 'Ascending max', 'Descending min', \n",
    "                                      'Descending max', 'Random', 'Random', 'Random', 'Random'], n_trials)\n",
    "\n",
    "    # Number of hidden layers\n",
    "    hidden_layers = getUniformList([2, 3, 4, 5], n_trials)\n",
    "    \n",
    "    # Create true/false variable for layers 3-5 based on number of layers\n",
    "    L3 = []\n",
    "    L4 = []\n",
    "    L5 = []\n",
    "    \n",
    "    for l in hidden_layers:\n",
    "        L3v = True\n",
    "        L4v = True\n",
    "        L5v = True\n",
    "        \n",
    "        if l < 5:\n",
    "            L5v = False\n",
    "        if l < 4:\n",
    "            L4v = False\n",
    "        if l < 3:\n",
    "            L3v = False\n",
    "            \n",
    "        L3.append(L3v)\n",
    "        L4.append(L4v)\n",
    "        L5.append(L5v)\n",
    "    \n",
    "\n",
    "    # Activation functions\n",
    "    activations = getUniformList(hp_ranges.get('Activations'), n_trials)\n",
    "\n",
    "    # Allow drop out layers?\n",
    "    allow_do = getUniformList([False, True, True, True], n_trials)\n",
    "\n",
    "    # Allow momentum?\n",
    "    allow_mom = getUniformList([False, True, True, True], n_trials)\n",
    "    \n",
    "    \n",
    "    # Make a sampler of dimension 1 for Sobol values\n",
    "    sampler_1 = Sobol(d=1, scramble=True)\n",
    "    \n",
    "    \n",
    "    # Get learning rate and momentum values\n",
    "    lr_Sobol = qmc.scale(sampler_1.random(n_trials), hp_ranges.get('Learning rate')[0], \n",
    "                         hp_ranges.get('Learning rate')[1])\n",
    "    bs_Sobol = qmc.scale(sampler_1.random(n_trials), hp_ranges.get('Batch size')[0], \n",
    "                         hp_ranges.get('Batch size')[1])\n",
    "    \n",
    "    # Lists to hold values for each model\n",
    "    LR = []\n",
    "    BS = []\n",
    "\n",
    "    # Sobol sequences are arrays within which each value is in a list, so we need to convert this to a single list\n",
    "    for i in lr_Sobol:\n",
    "        LR.append(i[0])\n",
    "        \n",
    "    for i in bs_Sobol:\n",
    "        # Batch size must be an integer\n",
    "        BS.append(int(round(i[0], 0)))\n",
    "    \n",
    "    # Put values in a dataframe so we can iterate through the dataframe to define the remaining values\n",
    "    # We set defaults at this stage for values we have not defined\n",
    "    trials = pd.DataFrame({'Structure': layer_structure, 'Number of hidden layers': hidden_layers, \n",
    "                           'Activation': activations, 'Allow dropout layers?': allow_do, \n",
    "                           'Allow momentum?': allow_mom, 'L3': L3, 'L4': L4, 'L5': L5, \n",
    "                           'D1': False, 'D2': False, 'D3': False, 'D4': False, 'D5': False, \n",
    "                           'L1N': 0, 'L2N': 0, 'L3N': 0, 'L4N': 0, 'L5N': 0, \n",
    "                           'D1R': 0, 'D2R': 0, 'D3R': 0, 'D4R': 0, 'D5R': 0, \n",
    "                           'Learning rate': LR, 'Momentum': 0.0, 'Batch size': BS})\n",
    "    \n",
    "    \n",
    "    # Momenum values\n",
    "    # We only want to set a non-zero momentum value where allow momentum = TRUE\n",
    "    trials_am = trials[trials['Allow momentum?']==True]\n",
    "    mom_Sobol = qmc.scale(sampler_1.random(len(trials_am)), hp_ranges.get('Momentum')[0], hp_ranges.get('Momentum')[1])\n",
    "    count = 0\n",
    "    for i in trials_am.index:\n",
    "        trials.loc[i, 'Momentum'] = mom_Sobol[count][0]\n",
    "        count += 1\n",
    "    \n",
    "    \n",
    "    # Whether to add a dropout layer after a dense layer\n",
    "    \n",
    "    # Get subsets where layer size = 3, 4, 5\n",
    "    trials_L3 = trials[trials['L3']==True]\n",
    "    trials_L4 = trials[trials['L4']==True]\n",
    "    trials_L5 = trials[trials['L5']==True]\n",
    "    \n",
    "    # Individual dropout layers - True/False distribution\n",
    "    DO1 = getUniformList([False, True], n_trials)\n",
    "    DO2 = getUniformList([False, True], n_trials)\n",
    "    DO3 = getUniformList([False, True], len(trials_L3))\n",
    "    DO4 = getUniformList([False, True], len(trials_L4))\n",
    "    DO5 = getUniformList([False, True], len(trials_L5))\n",
    "    \n",
    "    for i in trials.index:\n",
    "        trials.loc[i, 'D1'] = DO1[i]\n",
    "        trials.loc[i, 'D2'] = DO2[i]\n",
    "    \n",
    "    # For the remaining layers need to set a count to iterate through the list \n",
    "    count = 0\n",
    "    for i in trials_L3.index:\n",
    "        trials.loc[i, 'D3'] = DO3[count]\n",
    "        count += 1\n",
    "    \n",
    "    count = 0\n",
    "    for i in trials_L4.index:\n",
    "        trials.loc[i, 'D4'] = DO4[count]\n",
    "        count += 1\n",
    "        \n",
    "    count = 0\n",
    "    for i in trials_L5.index:\n",
    "        trials.loc[i, 'D5'] = DO5[count]\n",
    "        count += 1\n",
    "    \n",
    "    \n",
    "    # Dropout layer rate\n",
    "    trials_D1 = trials[trials['D1']==True]\n",
    "    trials_D2 = trials[trials['D2']==True]\n",
    "    trials_D3 = trials[trials['D3']==True]\n",
    "    trials_D4 = trials[trials['D4']==True]\n",
    "    trials_D5 = trials[trials['D5']==True]\n",
    "    \n",
    "    # Define dropout rate range\n",
    "    do_lower = hp_ranges.get('Dropout rate')[0]\n",
    "    do_upper = hp_ranges.get('Dropout rate')[1]\n",
    "    \n",
    "    # Get values\n",
    "    DO1R = qmc.scale(sampler_1.random(len(trials_D1)), do_lower, do_upper)\n",
    "    DO2R = qmc.scale(sampler_1.random(len(trials_D2)), do_lower, do_upper)\n",
    "    DO3R = qmc.scale(sampler_1.random(len(trials_D3)), do_lower, do_upper)\n",
    "    DO4R = qmc.scale(sampler_1.random(len(trials_D4)), do_lower, do_upper)\n",
    "    DO5R = qmc.scale(sampler_1.random(len(trials_D5)), do_lower, do_upper)\n",
    "    \n",
    "    # Append values to dataframe\n",
    "    count = 0\n",
    "    for i in trials_D1.index:\n",
    "        trials.loc[i, 'D1R'] = DO1R[count]\n",
    "        count += 1\n",
    "    \n",
    "    count = 0\n",
    "    for i in trials_D2.index:\n",
    "        trials.loc[i, 'D2R'] = DO2R[count]\n",
    "        count += 1\n",
    "    \n",
    "    count = 0\n",
    "    for i in trials_D3.index:\n",
    "        trials.loc[i, 'D3R'] = DO3R[count]\n",
    "        count += 1\n",
    "    \n",
    "    count = 0\n",
    "    for i in trials_D4.index:\n",
    "        trials.loc[i, 'D4R'] = DO4R[count]\n",
    "        count += 1\n",
    "        \n",
    "    count = 0\n",
    "    for i in trials_D5.index:\n",
    "        trials.loc[i, 'D5R'] = DO5R[count]\n",
    "        count += 1\n",
    "\n",
    "    \n",
    "    # Get later sizes based on layer structure\n",
    "    # We want to make sure each set of models with a particular structure have a Sobol distribution\n",
    "\n",
    "    # Define layer size range\n",
    "    ls_lower = hp_ranges.get('Layer sizes')[0]\n",
    "    ls_upper = hp_ranges.get('Layer sizes')[1]\n",
    "    \n",
    "    # Uniform\n",
    "    trials_uniform = trials[trials['Structure']=='Uniform']\n",
    "    uniform_Sobol = qmc.scale(sampler_1.random(len(trials_uniform)), ls_lower, ls_upper)\n",
    "    count = 0\n",
    "    for i in trials_uniform.index:\n",
    "        for layer in ['L1N', 'L2N', 'L3N', 'L4N', 'L5N']:\n",
    "            trials.loc[i, layer] = int(round(uniform_Sobol[count][0], 0))\n",
    "        count += 1\n",
    "    \n",
    "    # Ascending min - quasirandom number is the smallest value in an ascending network\n",
    "    trials_ascmin = trials[trials['Structure']=='Ascending min']\n",
    "    ascmin_Sobol = qmc.scale(sampler_1.random(len(trials_ascmin)), ls_lower, ls_upper)\n",
    "    count = 0\n",
    "    for i in trials_ascmin.index:\n",
    "        layer_count = 1\n",
    "        for layer in ['L1N', 'L2N', 'L3N', 'L4N', 'L5N']:\n",
    "            trials.loc[i, layer] = int(round(ascmin_Sobol[count][0], 0)) * layer_count\n",
    "            layer_count += 1\n",
    "        count += 1\n",
    "        \n",
    "    # Descending max - quasirandom number is the largest value in an descending network\n",
    "    trials_desmax = trials[trials['Structure']=='Descending max']\n",
    "    desmax_Sobol = qmc.scale(sampler_1.random(len(trials_desmax)), ls_lower, ls_upper)\n",
    "    count = 0\n",
    "    for i in trials_desmax.index:\n",
    "        layer_count = 1\n",
    "        for layer in ['L1N', 'L2N', 'L3N', 'L4N', 'L5N']:\n",
    "            trials.loc[i, layer] = int(round(desmax_Sobol[count][0] / layer_count, 0))\n",
    "            layer_count += 1\n",
    "        count += 1\n",
    "        \n",
    "    # For 'Ascending max' and 'Descending min' we have to consider number of layers \n",
    "    \n",
    "    # Ascending max - quasirandom number is the largest value in an ascending network\n",
    "    trials_ascmax = trials[trials['Structure']=='Ascending max']\n",
    "    ascmax_Sobol = qmc.scale(sampler_1.random(len(trials_ascmax)), ls_lower, ls_upper)\n",
    "    count = 0\n",
    "    for i in trials_ascmax.index:\n",
    "        num_layers = trials_ascmax['Number of hidden layers'][i]\n",
    "        seed_value = int(round(ascmax_Sobol[count][0], 0))\n",
    "        if num_layers == 5:\n",
    "            layer_list = ['L5N', 'L4N', 'L3N', 'L2N', 'L1N']\n",
    "        elif num_layers == 4:\n",
    "            layer_list = ['L4N', 'L3N', 'L2N', 'L1N']\n",
    "        elif num_layers == 3:\n",
    "            layer_list = ['L3N', 'L2N', 'L1N']\n",
    "        else:\n",
    "            layer_list = ['L2N', 'L1N']\n",
    "        \n",
    "        layer_count = 1\n",
    "        for layer in layer_list:\n",
    "            trials.loc[i, layer] = int(round(ascmax_Sobol[count][0] / layer_count, 0))\n",
    "            layer_count += 1\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    # Descending min - quasirandom number is the smallest value in an descending network\n",
    "    trials_desmin = trials[trials['Structure']=='Descending min']\n",
    "    desmin_Sobol = qmc.scale(sampler_1.random(len(trials_desmin)), ls_lower, ls_upper)\n",
    "    count = 0\n",
    "    for i in trials_desmin.index:\n",
    "        num_layers = trials_desmin['Number of hidden layers'][i]\n",
    "        seed_value = int(round(desmin_Sobol[count][0], 0))\n",
    "        if num_layers == 5:\n",
    "            layer_list = ['L5N', 'L4N', 'L3N', 'L2N', 'L1N']\n",
    "        elif num_layers == 4:\n",
    "            layer_list = ['L4N', 'L3N', 'L2N', 'L1N']\n",
    "        elif num_layers == 3:\n",
    "            layer_list = ['L3N', 'L2N', 'L1N']\n",
    "        else:\n",
    "            layer_list = ['L2N', 'L1N']\n",
    "        \n",
    "        layer_count = 1\n",
    "        for layer in layer_list:\n",
    "            trials.loc[i, layer] = int(round(desmin_Sobol[count][0], 0)) * layer_count\n",
    "            layer_count += 1\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    # Random structure \n",
    "    sampler_5 = Sobol(d=5, scramble=True)\n",
    "    trials_random = trials[trials['Structure']=='Random']\n",
    "    random_Sobol = qmc.scale(sampler_5.random(len(trials_random)), [ls_lower for x in range(0,5)], [ls_upper for x in range(0,5)])\n",
    "    count = 0\n",
    "    for i in trials_random.index:\n",
    "        layer_count = 0\n",
    "        for layer in ['L1N', 'L2N', 'L3N', 'L4N', 'L5N']:\n",
    "            trials.loc[i, layer] = int(round(random_Sobol[count][layer_count], 0))\n",
    "            layer_count += 1\n",
    "        count += 1\n",
    "    \n",
    "    # Run each model and save min loss + max accuracy\n",
    "    for i in trials.index:\n",
    "        print(\"Trialling model\", i+1, \"of\", len(trials))\n",
    "\n",
    "        # Create and fit model, get loss and accuracy\n",
    "        hist_dict = modelBuilder(L1N=trials['L1N'][i], \n",
    "                                 D1=trials['D1'][i], D1R=trials['D1R'][i],\n",
    "                                 L2N=trials['L2N'][i], \n",
    "                                 D2=trials['D2'][i], D2R=trials['D2R'][i],\n",
    "                                 L3=trials['L3'][i], L3N=trials['L3N'][i], \n",
    "                                 D3=trials['D3'][i], D3R=trials['D3R'][i],\n",
    "                                 L4=trials['L4'][i], L4N=trials['L4N'][i], \n",
    "                                 D4=trials['D4'][i], D4R=trials['D4R'][i],\n",
    "                                 L5=trials['L5'][i], L5N=trials['L5N'][i], \n",
    "                                 D5=trials['D5'][i], D5R=trials['D5R'][i],\n",
    "                                 OLR=trials['Learning rate'][i], OM=trials['Momentum'][i], \n",
    "                                 AVN=trials['Activation'][i], BS=trials['Batch size'][i],\n",
    "                                 x_train=x_train, y_train=y_train, x_valid=x_valid, y_valid=y_valid,\n",
    "                                 model_type='Trials')\n",
    "\n",
    "        valid_loss = pd.DataFrame({'Epoch': range(1, len(hist_dict['val_loss'])+1), 'Loss': hist_dict['val_loss']})\n",
    "        valid_acc = pd.DataFrame({'Epoch': range(1, len(hist_dict['val_accuracy'])+1), 'Accuracy': hist_dict['val_accuracy']})\n",
    "        \n",
    "        trials.loc[i, 'Min val loss'] = valid_loss.sort_values(by=['Loss']).head(1).reset_index()['Loss'][0]\n",
    "        trials.loc[i, 'Min val loss epoch'] = valid_loss.sort_values(by=['Loss']).head(1).reset_index()['Epoch'][0]\n",
    "        trials.loc[i, 'Max val accuracy'] = valid_acc.sort_values(by=['Accuracy'], ascending=False).head(1).reset_index()['Accuracy'][0]\n",
    "        trials.loc[i, 'Max val accuracy epoch'] = valid_acc.sort_values(by=['Accuracy'], ascending=False).head(1).reset_index()['Epoch'][0]\n",
    "        \n",
    "        best_trial = trials.sort_values(by=['Min val loss']).head(1)\n",
    "    \n",
    "    return best_trial, trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e02b4",
   "metadata": {
    "id": "7fb70c8d"
   },
   "source": [
    "# 3 Running the program <a class=\"anchor\" id=\"run\"></a>\n",
    "\n",
    "We can now define our hyperparameter ranges and run hyperparameter optimisation program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c43df7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trialling model 1 of 300\n",
      "Trialling model 2 of 300\n",
      "Trialling model 3 of 300\n",
      "Trialling model 4 of 300\n",
      "Trialling model 5 of 300\n",
      "Trialling model 6 of 300\n",
      "Trialling model 7 of 300\n",
      "Trialling model 8 of 300\n",
      "Trialling model 9 of 300\n",
      "Trialling model 10 of 300\n",
      "Trialling model 11 of 300\n",
      "Trialling model 12 of 300\n",
      "Trialling model 13 of 300\n",
      "Trialling model 14 of 300\n",
      "Trialling model 15 of 300\n",
      "Trialling model 16 of 300\n",
      "Trialling model 17 of 300\n",
      "Trialling model 18 of 300\n",
      "Trialling model 19 of 300\n",
      "Trialling model 20 of 300\n",
      "Trialling model 21 of 300\n",
      "Trialling model 22 of 300\n",
      "Trialling model 23 of 300\n",
      "Trialling model 24 of 300\n",
      "Trialling model 25 of 300\n",
      "Trialling model 26 of 300\n",
      "Trialling model 27 of 300\n",
      "Trialling model 28 of 300\n",
      "Trialling model 29 of 300\n",
      "Trialling model 30 of 300\n",
      "Trialling model 31 of 300\n",
      "Trialling model 32 of 300\n",
      "Trialling model 33 of 300\n",
      "Trialling model 34 of 300\n",
      "Trialling model 35 of 300\n",
      "Trialling model 36 of 300\n",
      "Trialling model 37 of 300\n",
      "Trialling model 38 of 300\n",
      "Trialling model 39 of 300\n",
      "Trialling model 40 of 300\n",
      "Trialling model 41 of 300\n",
      "Trialling model 42 of 300\n",
      "Trialling model 43 of 300\n",
      "Trialling model 44 of 300\n",
      "Trialling model 45 of 300\n",
      "Trialling model 46 of 300\n",
      "Trialling model 47 of 300\n",
      "Trialling model 48 of 300\n",
      "Trialling model 49 of 300\n",
      "Trialling model 50 of 300\n",
      "Trialling model 51 of 300\n",
      "Trialling model 52 of 300\n",
      "Trialling model 53 of 300\n",
      "Trialling model 54 of 300\n",
      "Trialling model 55 of 300\n",
      "Trialling model 56 of 300\n",
      "Trialling model 57 of 300\n",
      "Trialling model 58 of 300\n",
      "Trialling model 59 of 300\n",
      "Trialling model 60 of 300\n",
      "Trialling model 61 of 300\n",
      "Trialling model 62 of 300\n",
      "Trialling model 63 of 300\n",
      "Trialling model 64 of 300\n",
      "Trialling model 65 of 300\n",
      "Trialling model 66 of 300\n",
      "Trialling model 67 of 300\n",
      "Trialling model 68 of 300\n",
      "Trialling model 69 of 300\n",
      "Trialling model 70 of 300\n",
      "Trialling model 71 of 300\n",
      "Trialling model 72 of 300\n",
      "Trialling model 73 of 300\n",
      "Trialling model 74 of 300\n",
      "Trialling model 75 of 300\n",
      "Trialling model 76 of 300\n",
      "Trialling model 77 of 300\n",
      "Trialling model 78 of 300\n",
      "Trialling model 79 of 300\n",
      "Trialling model 80 of 300\n",
      "Trialling model 81 of 300\n",
      "Trialling model 82 of 300\n",
      "Trialling model 83 of 300\n",
      "Trialling model 84 of 300\n",
      "Trialling model 85 of 300\n",
      "Trialling model 86 of 300\n",
      "Trialling model 87 of 300\n",
      "Trialling model 88 of 300\n",
      "Trialling model 89 of 300\n",
      "Trialling model 90 of 300\n",
      "Trialling model 91 of 300\n",
      "Trialling model 92 of 300\n",
      "Trialling model 93 of 300\n",
      "Trialling model 94 of 300\n",
      "Trialling model 95 of 300\n",
      "Trialling model 96 of 300\n",
      "Trialling model 97 of 300\n",
      "Trialling model 98 of 300\n",
      "Trialling model 99 of 300\n",
      "Trialling model 100 of 300\n",
      "Trialling model 101 of 300\n",
      "Trialling model 102 of 300\n",
      "Trialling model 103 of 300\n",
      "Trialling model 104 of 300\n",
      "Trialling model 105 of 300\n",
      "Trialling model 106 of 300\n",
      "Trialling model 107 of 300\n",
      "Trialling model 108 of 300\n",
      "Trialling model 109 of 300\n",
      "Trialling model 110 of 300\n",
      "Trialling model 111 of 300\n",
      "Trialling model 112 of 300\n",
      "Trialling model 113 of 300\n",
      "Trialling model 114 of 300\n",
      "Trialling model 115 of 300\n",
      "Trialling model 116 of 300\n",
      "Trialling model 117 of 300\n",
      "Trialling model 118 of 300\n",
      "Trialling model 119 of 300\n",
      "Trialling model 120 of 300\n",
      "Trialling model 121 of 300\n",
      "Trialling model 122 of 300\n",
      "Trialling model 123 of 300\n",
      "Trialling model 124 of 300\n",
      "Trialling model 125 of 300\n",
      "Trialling model 126 of 300\n",
      "Trialling model 127 of 300\n",
      "Trialling model 128 of 300\n",
      "Trialling model 129 of 300\n",
      "Trialling model 130 of 300\n",
      "Trialling model 131 of 300\n",
      "Trialling model 132 of 300\n",
      "Trialling model 133 of 300\n",
      "Trialling model 134 of 300\n",
      "Trialling model 135 of 300\n",
      "Trialling model 136 of 300\n",
      "Trialling model 137 of 300\n",
      "Trialling model 138 of 300\n",
      "Trialling model 139 of 300\n",
      "Trialling model 140 of 300\n",
      "Trialling model 141 of 300\n",
      "Trialling model 142 of 300\n",
      "Trialling model 143 of 300\n",
      "Trialling model 144 of 300\n",
      "Trialling model 145 of 300\n",
      "Trialling model 146 of 300\n",
      "Trialling model 147 of 300\n",
      "Trialling model 148 of 300\n",
      "Trialling model 149 of 300\n",
      "Trialling model 150 of 300\n",
      "Trialling model 151 of 300\n",
      "Trialling model 152 of 300\n",
      "Trialling model 153 of 300\n",
      "Trialling model 154 of 300\n",
      "Trialling model 155 of 300\n",
      "Trialling model 156 of 300\n",
      "Trialling model 157 of 300\n",
      "Trialling model 158 of 300\n",
      "Trialling model 159 of 300\n",
      "Trialling model 160 of 300\n",
      "Trialling model 161 of 300\n",
      "Trialling model 162 of 300\n",
      "Trialling model 163 of 300\n",
      "Trialling model 164 of 300\n",
      "Trialling model 165 of 300\n",
      "Trialling model 166 of 300\n",
      "Trialling model 167 of 300\n",
      "Trialling model 168 of 300\n",
      "Trialling model 169 of 300\n",
      "Trialling model 170 of 300\n",
      "Trialling model 171 of 300\n",
      "Trialling model 172 of 300\n",
      "Trialling model 173 of 300\n",
      "Trialling model 174 of 300\n",
      "Trialling model 175 of 300\n",
      "Trialling model 176 of 300\n",
      "Trialling model 177 of 300\n",
      "Trialling model 178 of 300\n",
      "Trialling model 179 of 300\n",
      "Trialling model 180 of 300\n",
      "Trialling model 181 of 300\n",
      "Trialling model 182 of 300\n",
      "Trialling model 183 of 300\n",
      "Trialling model 184 of 300\n",
      "Trialling model 185 of 300\n",
      "Trialling model 186 of 300\n",
      "Trialling model 187 of 300\n",
      "Trialling model 188 of 300\n",
      "Trialling model 189 of 300\n",
      "Trialling model 190 of 300\n",
      "Trialling model 191 of 300\n",
      "Trialling model 192 of 300\n",
      "Trialling model 193 of 300\n",
      "Trialling model 194 of 300\n",
      "Trialling model 195 of 300\n",
      "Trialling model 196 of 300\n",
      "Trialling model 197 of 300\n",
      "Trialling model 198 of 300\n",
      "Trialling model 199 of 300\n",
      "Trialling model 200 of 300\n",
      "Trialling model 201 of 300\n",
      "Trialling model 202 of 300\n",
      "Trialling model 203 of 300\n",
      "Trialling model 204 of 300\n",
      "Trialling model 205 of 300\n",
      "Trialling model 206 of 300\n",
      "Trialling model 207 of 300\n",
      "Trialling model 208 of 300\n",
      "Trialling model 209 of 300\n",
      "Trialling model 210 of 300\n",
      "Trialling model 211 of 300\n",
      "Trialling model 212 of 300\n",
      "Trialling model 213 of 300\n",
      "Trialling model 214 of 300\n",
      "Trialling model 215 of 300\n",
      "Trialling model 216 of 300\n",
      "Trialling model 217 of 300\n",
      "Trialling model 218 of 300\n",
      "Trialling model 219 of 300\n",
      "Trialling model 220 of 300\n",
      "Trialling model 221 of 300\n",
      "Trialling model 222 of 300\n",
      "Trialling model 223 of 300\n",
      "Trialling model 224 of 300\n",
      "Trialling model 225 of 300\n",
      "Trialling model 226 of 300\n",
      "Trialling model 227 of 300\n",
      "Trialling model 228 of 300\n",
      "Trialling model 229 of 300\n",
      "Trialling model 230 of 300\n",
      "Trialling model 231 of 300\n",
      "Trialling model 232 of 300\n",
      "Trialling model 233 of 300\n",
      "Trialling model 234 of 300\n",
      "Trialling model 235 of 300\n",
      "Trialling model 236 of 300\n",
      "Trialling model 237 of 300\n",
      "Trialling model 238 of 300\n",
      "Trialling model 239 of 300\n",
      "Trialling model 240 of 300\n",
      "Trialling model 241 of 300\n",
      "Trialling model 242 of 300\n",
      "Trialling model 243 of 300\n",
      "Trialling model 244 of 300\n",
      "Trialling model 245 of 300\n",
      "Trialling model 246 of 300\n",
      "Trialling model 247 of 300\n",
      "Trialling model 248 of 300\n",
      "Trialling model 249 of 300\n",
      "Trialling model 250 of 300\n",
      "Trialling model 251 of 300\n",
      "Trialling model 252 of 300\n",
      "Trialling model 253 of 300\n",
      "Trialling model 254 of 300\n",
      "Trialling model 255 of 300\n",
      "Trialling model 256 of 300\n",
      "Trialling model 257 of 300\n",
      "Trialling model 258 of 300\n",
      "Trialling model 259 of 300\n",
      "Trialling model 260 of 300\n",
      "Trialling model 261 of 300\n",
      "Trialling model 262 of 300\n",
      "Trialling model 263 of 300\n",
      "Trialling model 264 of 300\n",
      "Trialling model 265 of 300\n",
      "Trialling model 266 of 300\n",
      "Trialling model 267 of 300\n",
      "Trialling model 268 of 300\n",
      "Trialling model 269 of 300\n",
      "Trialling model 270 of 300\n",
      "Trialling model 271 of 300\n",
      "Trialling model 272 of 300\n",
      "Trialling model 273 of 300\n",
      "Trialling model 274 of 300\n",
      "Trialling model 275 of 300\n",
      "Trialling model 276 of 300\n",
      "Trialling model 277 of 300\n",
      "Trialling model 278 of 300\n",
      "Trialling model 279 of 300\n",
      "Trialling model 280 of 300\n",
      "Trialling model 281 of 300\n",
      "Trialling model 282 of 300\n",
      "Trialling model 283 of 300\n",
      "Trialling model 284 of 300\n",
      "Trialling model 285 of 300\n",
      "Trialling model 286 of 300\n",
      "Trialling model 287 of 300\n",
      "Trialling model 288 of 300\n",
      "Trialling model 289 of 300\n",
      "Trialling model 290 of 300\n",
      "Trialling model 291 of 300\n",
      "Trialling model 292 of 300\n",
      "Trialling model 293 of 300\n",
      "Trialling model 294 of 300\n",
      "Trialling model 295 of 300\n",
      "Trialling model 296 of 300\n",
      "Trialling model 297 of 300\n",
      "Trialling model 298 of 300\n",
      "Trialling model 299 of 300\n",
      "Trialling model 300 of 300\n",
      "Time taken: 7521.050108671188 seconds\n"
     ]
    }
   ],
   "source": [
    "hp_ranges = {'Activations': ['relu', 'tanh', 'sigmoid'], 'Layer sizes': [20, 1500], 'Dropout rate': [0, 0.7],\n",
    "            'Learning rate': [0.0005, 0.005], 'Momentum': [0, 1], 'Batch size': [5, 600]}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_trial, all_trials = QRSearch(n_trials=300, hp_ranges=hp_ranges, x_train=x_train, y_train=y_train, \n",
    "                                  x_valid=x_valid, y_valid=y_valid)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print('Time taken:', time_taken, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54b5ba23",
   "metadata": {
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1643764479805,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "ZAga2pHI3vJd"
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "\n",
    "best_trial.to_csv('best_trial.csv')\n",
    "!cp best_trial.csv \"drive/My Drive/\"\n",
    "\n",
    "all_trials.to_csv('all_trials.csv')\n",
    "!cp all_trials.csv \"drive/My Drive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce718a1",
   "metadata": {},
   "source": [
    "We can now see the model which performed best at trial stage, and which loss/accuracy it achieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2542cd7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1643764479809,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "ee2edd00",
    "outputId": "713a7c3d-e674-4fe7-8a11-af7fde9d6608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure : Ascending max\n",
      "Number of hidden layers : 2\n",
      "Activation : sigmoid\n",
      "Allow dropout layers? : True\n",
      "Allow momentum? : True\n",
      "L3 : False\n",
      "L4 : False\n",
      "L5 : False\n",
      "D1 : True\n",
      "D2 : False\n",
      "D3 : False\n",
      "D4 : False\n",
      "D5 : False\n",
      "L1N : 163\n",
      "L2N : 325\n",
      "L3N : 0\n",
      "L4N : 0\n",
      "L5N : 0\n",
      "D1R : 0.1725827321410179\n",
      "D2R : 0.0\n",
      "D3R : 0.0\n",
      "D4R : 0.0\n",
      "D5R : 0.0\n",
      "Learning rate : 0.0030597063601017003\n",
      "Momentum : 0.19803881645202637\n",
      "Batch size : 544\n",
      "Min val loss : 0.28139692544937134\n",
      "Min val loss epoch : 44.0\n",
      "Max val accuracy : 0.8985000252723694\n",
      "Max val accuracy epoch : 44.0\n"
     ]
    }
   ],
   "source": [
    "best_trial.reset_index(inplace=True, drop=True)\n",
    "\n",
    "for col in best_trial:\n",
    "    print(col, ':', best_trial[col][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476d74a",
   "metadata": {
    "id": "acbb1d50"
   },
   "source": [
    "# 4 Evaluating the best performing model using test data <a class=\"anchor\" id=\"test\"></a>\n",
    "\n",
    "We will now evaulate our best performing model using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d2b29ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22693,
     "status": "ok",
     "timestamp": 1643764502497,
     "user": {
      "displayName": "Alexandra Stepanenko",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09067281291667623362"
     },
     "user_tz": 0
    },
    "id": "33ad95b6",
    "outputId": "ad36dc15-b3c2-458f-f45d-d3f1858d1417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/44\n",
      "111/111 [==============================] - 1s 3ms/step - loss: 0.8926 - accuracy: 0.6723\n",
      "Epoch 2/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.8015\n",
      "Epoch 3/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.4709 - accuracy: 0.8271\n",
      "Epoch 4/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.4353 - accuracy: 0.8379\n",
      "Epoch 5/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.4103 - accuracy: 0.8483\n",
      "Epoch 6/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.3946 - accuracy: 0.8543\n",
      "Epoch 7/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.3802 - accuracy: 0.8582\n",
      "Epoch 8/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.3653 - accuracy: 0.8643\n",
      "Epoch 9/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.3540 - accuracy: 0.8692\n",
      "Epoch 10/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.3455 - accuracy: 0.8716\n",
      "Epoch 11/44\n",
      "111/111 [==============================] - 0s 4ms/step - loss: 0.3337 - accuracy: 0.8765\n",
      "Epoch 12/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.8780\n",
      "Epoch 13/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.3176 - accuracy: 0.8817\n",
      "Epoch 14/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.3131 - accuracy: 0.8826\n",
      "Epoch 15/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.3061 - accuracy: 0.8870\n",
      "Epoch 16/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2994 - accuracy: 0.8876\n",
      "Epoch 17/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2970 - accuracy: 0.8884\n",
      "Epoch 18/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2905 - accuracy: 0.8912\n",
      "Epoch 19/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2845 - accuracy: 0.8932\n",
      "Epoch 20/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2801 - accuracy: 0.8938\n",
      "Epoch 21/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2768 - accuracy: 0.8961\n",
      "Epoch 22/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2753 - accuracy: 0.8958\n",
      "Epoch 23/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2673 - accuracy: 0.8988\n",
      "Epoch 24/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2638 - accuracy: 0.9002\n",
      "Epoch 25/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2608 - accuracy: 0.9018\n",
      "Epoch 26/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2571 - accuracy: 0.9023\n",
      "Epoch 27/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2539 - accuracy: 0.9034\n",
      "Epoch 28/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2492 - accuracy: 0.9056\n",
      "Epoch 29/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2477 - accuracy: 0.9053\n",
      "Epoch 30/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2453 - accuracy: 0.9069\n",
      "Epoch 31/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2399 - accuracy: 0.9083\n",
      "Epoch 32/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2387 - accuracy: 0.9094\n",
      "Epoch 33/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2365 - accuracy: 0.9101\n",
      "Epoch 34/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2350 - accuracy: 0.9091\n",
      "Epoch 35/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2322 - accuracy: 0.9115\n",
      "Epoch 36/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2308 - accuracy: 0.9115\n",
      "Epoch 37/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.9130\n",
      "Epoch 38/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2241 - accuracy: 0.9140\n",
      "Epoch 39/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2208 - accuracy: 0.9159\n",
      "Epoch 40/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2190 - accuracy: 0.9158\n",
      "Epoch 41/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2170 - accuracy: 0.9176\n",
      "Epoch 42/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.9170\n",
      "Epoch 43/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2119 - accuracy: 0.9184\n",
      "Epoch 44/44\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.9194\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3636 - accuracy: 0.8773\n",
      "The test loss was: 0.36361244320869446 \n",
      "The test accuracy was: 0.8773000240325928\n"
     ]
    }
   ],
   "source": [
    "modelBuilder(L1N=best_trial['L1N'][0], \n",
    "             D1=best_trial['D1'][0], D1R=best_trial['D1R'][0],\n",
    "             L2N=best_trial['L2N'][0], \n",
    "             D2=best_trial['D2'][0], D2R=best_trial['D2R'][0],\n",
    "             L3=best_trial['L3'][0], L3N=best_trial['L3N'][0], \n",
    "             D3=best_trial['D3'][0], D3R=best_trial['D3R'][0],\n",
    "             L4=best_trial['L4'][0], L4N=best_trial['L4N'][0], \n",
    "             D4=best_trial['D4'][0], D4R=best_trial['D4R'][0],\n",
    "             L5=best_trial['L5'][0], L5N=best_trial['L5N'][0], \n",
    "             D5=best_trial['D5'][0], D5R=best_trial['D5R'][0],\n",
    "             OLR=best_trial['Learning rate'][0], OM=best_trial['Momentum'][0], \n",
    "             AVN=best_trial['Activation'][0], BS=best_trial['Batch size'][0],\n",
    "             train_images=train_images, train_labels=train_labels, \n",
    "             test_images=test_images, test_labels=test_labels, test_epochs=int(best_trial['Max val accuracy epoch'][0]),\n",
    "             model_type='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa69d95",
   "metadata": {
    "id": "cf690c38"
   },
   "source": [
    "# 5 Conclusion and evaluation <a class=\"anchor\" id=\"conclusion\"></a>\n",
    "\n",
    "Using my hyperparameter optimisation program, the best model during trialing achieved a maximum validation accuracy of 0.899. In the test evaluation, the model achieved an accuracy of 0.877. \n",
    "\n",
    "Bergstra and Bengio [1] noted that random search was generally not as effective as a human expert using a combination of manual and grid search because a person learns from the results of previous experiments and is able to adapt their approach. Interestingly, I previously used the manual/grid approach to optimise a neural network on the same data set, and the maximum validation accuracy I achieved was also 0.899 - although it should be noted that once this program was built, this time I got to the same validation accuracy much quicker than through manual and grid search! \n",
    "\n",
    "There are now approaches such as Bayesian Optimisation, which use information from previous trials to attempt to identify which hyperparameter values may reduce the loss of a model, which have an advantage over random search methods.\n",
    "\n",
    "One of the elements I found difficult during this project was picking suitable ranges for hyperparameters when calling the function. On the one hand, it seems prudent to pick a wide range, in case your selection excludes the optimum for a hyperparameter. On the other hand, very large ranges may suffer from the grid search problem of missing the optimum in the gap between values. A potential improvement on the program I have written is: after a certain number of models have been trialed (for example, 100 models), for the program to restrict the hyperparameter ranges for the next set of models based on the hyperparameter values of the best performing models of the previous set. However, it is unclear how significant this reduction in range would be, and if there are any potential drawbacks to this method (for example, inadvertently excluding the optimum for a hyperparameter early on). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42501c",
   "metadata": {
    "id": "e2f1e840"
   },
   "source": [
    "# 6 References <a class=\"anchor\" id=\"references\"></a>\n",
    "\n",
    "[1] Bergstra, J., & Bengio, Y. (2012). Random Search for Hyper-Parameter Optimization. J. Mach. Learn. Res., 13, 281-305.  \n",
    "[2] https://scipy.github.io/devdocs/reference/generated/scipy.stats.qmc.Sobol.html#re15be05a07a0-3. \n",
    "\n",
    "Other resources used:\n",
    "\n",
    "* T. Blackwell, *Artificial Intelligence Lectures and Labs*, London: Goldsmiths,University of London, 2021.\n",
    "* F. Chollet, *Deep Learning with Python*, Shelter Island: Manning, 2018.\n",
    "* M. Nielsen, *Neural Networks and Deep Learning* [Online]. Avaliable: http://neuralnetworksanddeeplearning.com/\n",
    "* Zalando Research, *Fashion-MNIST* [Online]. Avaliable: https://github.com/zalandoresearch/fashion-mnist"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Coursework 2 - Quasi-Random Search for Hyperparameter Optimisation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
